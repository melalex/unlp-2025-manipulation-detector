{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base BERT implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from tokenizers import pre_tokenizers\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.util.torch_device import resolve_torch_device\n",
    "from src.data.span_detection_ds import ManipulationDetectionDataset\n",
    "from src.visualization.plot import plot_loss, plot_model_progress, plot_eval_loss\n",
    "from src.definitions import (\n",
    "    MODELS_FOLDER,\n",
    "    RAW_DATA_FOLDER,\n",
    "    REPORTS_FOLDER,\n",
    "    PROCESSED_DATA_FOLDER,\n",
    ")\n",
    "from src.visualization.ner import visualize_as_markdown_and_save\n",
    "from src.visualization.reporting import EvaluatingReport\n",
    "from src.model.span_detection_metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "device = resolve_torch_device()\n",
    "\n",
    "model_checkpoint = 'EvanD/xlm-roberta-base-ukrainian-ner-ukrner'\n",
    "\n",
    "epoch_time = int(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84156f88d3d046f3803ecf57d892ffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3822\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=512)\n",
    "\n",
    "dataset_blueprint = ManipulationDetectionDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    raw_path=RAW_DATA_FOLDER / \"span-detection.parquet\",\n",
    "    processed_path=PROCESSED_DATA_FOLDER / \"span-detection\",\n",
    "    seed=random_seed,\n",
    "    do_split=False,\n",
    ")\n",
    "\n",
    "dataset = dataset_blueprint.read()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_checkpoint, dataset_blueprint, device):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(dataset_blueprint.label2id),\n",
    "        id2label=dataset_blueprint.id2label,\n",
    "        label2id=dataset_blueprint.label2id,\n",
    "        #dropout=0.2,\n",
    "        #hidden_dropout_prob=0.2,\n",
    "        #attention_probs_dropout_prob=0.2,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_trainer(model, tokenizer, dataset, run_name):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODELS_FOLDER / \"manipulation-detector-bert-ner-checkpoint\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=4,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        seed=random_seed,\n",
    "        logging_strategy=\"epoch\",\n",
    "        run_name=run_name,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics(dataset_blueprint),\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at EvanD/xlm-roberta-base-ukrainian-ner-ukrner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 04:35, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token F1</th>\n",
       "      <th>Token Precision</th>\n",
       "      <th>Token Recall</th>\n",
       "      <th>Token Accuracy</th>\n",
       "      <th>Span F1</th>\n",
       "      <th>Span Precision</th>\n",
       "      <th>Span Recall</th>\n",
       "      <th>Span Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.420891</td>\n",
       "      <td>0.428558</td>\n",
       "      <td>0.731452</td>\n",
       "      <td>0.303061</td>\n",
       "      <td>0.796253</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.061372</td>\n",
       "      <td>0.070491</td>\n",
       "      <td>0.796253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.413646</td>\n",
       "      <td>0.553891</td>\n",
       "      <td>0.642718</td>\n",
       "      <td>0.486635</td>\n",
       "      <td>0.802386</td>\n",
       "      <td>0.098701</td>\n",
       "      <td>0.084737</td>\n",
       "      <td>0.118176</td>\n",
       "      <td>0.802386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.430890</td>\n",
       "      <td>0.560523</td>\n",
       "      <td>0.655280</td>\n",
       "      <td>0.489709</td>\n",
       "      <td>0.806412</td>\n",
       "      <td>0.110147</td>\n",
       "      <td>0.106520</td>\n",
       "      <td>0.114029</td>\n",
       "      <td>0.806412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.456074</td>\n",
       "      <td>0.559771</td>\n",
       "      <td>0.651807</td>\n",
       "      <td>0.490511</td>\n",
       "      <td>0.805502</td>\n",
       "      <td>0.115528</td>\n",
       "      <td>0.113636</td>\n",
       "      <td>0.117484</td>\n",
       "      <td>0.805502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at EvanD/xlm-roberta-base-ukrainian-ner-ukrner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 10:29, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token F1</th>\n",
       "      <th>Token Precision</th>\n",
       "      <th>Token Recall</th>\n",
       "      <th>Token Accuracy</th>\n",
       "      <th>Span F1</th>\n",
       "      <th>Span Precision</th>\n",
       "      <th>Span Recall</th>\n",
       "      <th>Span Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.433704</td>\n",
       "      <td>0.456241</td>\n",
       "      <td>0.686597</td>\n",
       "      <td>0.341625</td>\n",
       "      <td>0.795154</td>\n",
       "      <td>0.045997</td>\n",
       "      <td>0.039531</td>\n",
       "      <td>0.054990</td>\n",
       "      <td>0.795154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.428700</td>\n",
       "      <td>0.413642</td>\n",
       "      <td>0.589257</td>\n",
       "      <td>0.611731</td>\n",
       "      <td>0.568375</td>\n",
       "      <td>0.800673</td>\n",
       "      <td>0.085565</td>\n",
       "      <td>0.074885</td>\n",
       "      <td>0.099796</td>\n",
       "      <td>0.800673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.435633</td>\n",
       "      <td>0.529038</td>\n",
       "      <td>0.675702</td>\n",
       "      <td>0.434688</td>\n",
       "      <td>0.805311</td>\n",
       "      <td>0.093101</td>\n",
       "      <td>0.091870</td>\n",
       "      <td>0.094365</td>\n",
       "      <td>0.805311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.462730</td>\n",
       "      <td>0.554818</td>\n",
       "      <td>0.647225</td>\n",
       "      <td>0.485500</td>\n",
       "      <td>0.804006</td>\n",
       "      <td>0.103842</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.101833</td>\n",
       "      <td>0.804006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at EvanD/xlm-roberta-base-ukrainian-ner-ukrner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 10:23, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token F1</th>\n",
       "      <th>Token Precision</th>\n",
       "      <th>Token Recall</th>\n",
       "      <th>Token Accuracy</th>\n",
       "      <th>Span F1</th>\n",
       "      <th>Span Precision</th>\n",
       "      <th>Span Recall</th>\n",
       "      <th>Span Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.515314</td>\n",
       "      <td>0.213322</td>\n",
       "      <td>0.736304</td>\n",
       "      <td>0.124729</td>\n",
       "      <td>0.753121</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.053512</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.753121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.476391</td>\n",
       "      <td>0.599340</td>\n",
       "      <td>0.558795</td>\n",
       "      <td>0.646229</td>\n",
       "      <td>0.768131</td>\n",
       "      <td>0.100780</td>\n",
       "      <td>0.086242</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.768131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.491521</td>\n",
       "      <td>0.582158</td>\n",
       "      <td>0.587029</td>\n",
       "      <td>0.577368</td>\n",
       "      <td>0.777579</td>\n",
       "      <td>0.101638</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>0.109668</td>\n",
       "      <td>0.777579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.546326</td>\n",
       "      <td>0.534814</td>\n",
       "      <td>0.635026</td>\n",
       "      <td>0.461919</td>\n",
       "      <td>0.784353</td>\n",
       "      <td>0.098981</td>\n",
       "      <td>0.099853</td>\n",
       "      <td>0.098124</td>\n",
       "      <td>0.784353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at EvanD/xlm-roberta-base-ukrainian-ner-ukrner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='768' max='768' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [768/768 07:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token F1</th>\n",
       "      <th>Token Precision</th>\n",
       "      <th>Token Recall</th>\n",
       "      <th>Token Accuracy</th>\n",
       "      <th>Span F1</th>\n",
       "      <th>Span Precision</th>\n",
       "      <th>Span Recall</th>\n",
       "      <th>Span Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.482712</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.725756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.450879</td>\n",
       "      <td>0.610255</td>\n",
       "      <td>0.581983</td>\n",
       "      <td>0.641414</td>\n",
       "      <td>0.774799</td>\n",
       "      <td>0.097301</td>\n",
       "      <td>0.083716</td>\n",
       "      <td>0.116147</td>\n",
       "      <td>0.774799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.365200</td>\n",
       "      <td>0.447574</td>\n",
       "      <td>0.608991</td>\n",
       "      <td>0.600857</td>\n",
       "      <td>0.617347</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.120747</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>0.128187</td>\n",
       "      <td>0.782094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.316500</td>\n",
       "      <td>0.499090</td>\n",
       "      <td>0.569583</td>\n",
       "      <td>0.631761</td>\n",
       "      <td>0.518548</td>\n",
       "      <td>0.784581</td>\n",
       "      <td>0.113510</td>\n",
       "      <td>0.111644</td>\n",
       "      <td>0.115439</td>\n",
       "      <td>0.784581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at EvanD/xlm-roberta-base-ukrainian-ner-ukrner and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([9, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='764' max='764' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [764/764 04:41, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Token F1</th>\n",
       "      <th>Token Precision</th>\n",
       "      <th>Token Recall</th>\n",
       "      <th>Token Accuracy</th>\n",
       "      <th>Span F1</th>\n",
       "      <th>Span Precision</th>\n",
       "      <th>Span Recall</th>\n",
       "      <th>Span Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.410720</td>\n",
       "      <td>0.575988</td>\n",
       "      <td>0.620139</td>\n",
       "      <td>0.537707</td>\n",
       "      <td>0.802330</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.072102</td>\n",
       "      <td>0.093057</td>\n",
       "      <td>0.802330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.412751</td>\n",
       "      <td>0.588290</td>\n",
       "      <td>0.640061</td>\n",
       "      <td>0.544267</td>\n",
       "      <td>0.809786</td>\n",
       "      <td>0.100492</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>0.809786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.351700</td>\n",
       "      <td>0.410468</td>\n",
       "      <td>0.579690</td>\n",
       "      <td>0.649232</td>\n",
       "      <td>0.523605</td>\n",
       "      <td>0.810413</td>\n",
       "      <td>0.109429</td>\n",
       "      <td>0.105368</td>\n",
       "      <td>0.113815</td>\n",
       "      <td>0.810413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.300200</td>\n",
       "      <td>0.442643</td>\n",
       "      <td>0.570713</td>\n",
       "      <td>0.637092</td>\n",
       "      <td>0.516861</td>\n",
       "      <td>0.805851</td>\n",
       "      <td>0.112817</td>\n",
       "      <td>0.112536</td>\n",
       "      <td>0.113099</td>\n",
       "      <td>0.805851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 5\n",
    "length = len(dataset)\n",
    "fold_length = length // k\n",
    "eval_metrics = []\n",
    "for i in range(k):\n",
    "    start = i * fold_length\n",
    "    end = (i + 1) * fold_length\n",
    "    if i == k - 1:\n",
    "        end = length\n",
    "\n",
    "    train_idx = list(range(0, start)) + list(range(end, length))\n",
    "    test_idx = list(range(start, end))\n",
    "\n",
    "    train_dataset = dataset.select(train_idx)\n",
    "    test_dataset = dataset.select(test_idx)\n",
    "\n",
    "    model = get_model(model_checkpoint, dataset_blueprint, device)\n",
    "    trainer = get_trainer(model, tokenizer, {\"train\": train_dataset, \"test\": test_dataset}, f\"EvanD/xlm-roberta-base-ukrainian-ner-ukrner-{epoch_time}-fold-{i}\")\n",
    "\n",
    "    trainer.train()\n",
    "    evaluation_feedback = trainer.evaluate()\n",
    "\n",
    "    eval_metrics.append(evaluation_feedback)\n",
    "\n",
    "    # trainer.save_model(MODELS_FOLDER / f\"manipulation-detector-bert-ner-{epoch_time}-fold-{i}\")\n",
    "    # tokenizer.save_pretrained(MODELS_FOLDER / f\"manipulation-detector-bert-ner-{epoch_time}-fold-{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_token_f1</th>\n",
       "      <th>eval_token_precision</th>\n",
       "      <th>eval_token_recall</th>\n",
       "      <th>eval_token_accuracy</th>\n",
       "      <th>eval_span_f1</th>\n",
       "      <th>eval_span_precision</th>\n",
       "      <th>eval_span_recall</th>\n",
       "      <th>eval_span_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.481373</td>\n",
       "      <td>0.55794</td>\n",
       "      <td>0.640582</td>\n",
       "      <td>0.494668</td>\n",
       "      <td>0.796859</td>\n",
       "      <td>0.108936</td>\n",
       "      <td>0.10872</td>\n",
       "      <td>0.109196</td>\n",
       "      <td>0.796859</td>\n",
       "      <td>13.88224</td>\n",
       "      <td>80.5992</td>\n",
       "      <td>5.0592</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eval_loss  eval_token_f1  eval_token_precision  eval_token_recall  \\\n",
       "0   0.481373        0.55794              0.640582           0.494668   \n",
       "\n",
       "   eval_token_accuracy  eval_span_f1  eval_span_precision  eval_span_recall  \\\n",
       "0             0.796859      0.108936              0.10872          0.109196   \n",
       "\n",
       "   eval_span_accuracy  eval_runtime  eval_samples_per_second  \\\n",
       "0            0.796859      13.88224                  80.5992   \n",
       "\n",
       "   eval_steps_per_second  epoch  \n",
       "0                 5.0592    4.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_metrics_avg = {\n",
    "    \"eval_loss\": sum([m[\"eval_loss\"] for m in eval_metrics]) / k,\n",
    "    \"eval_token_f1\": sum([m[\"eval_token_f1\"] for m in eval_metrics]) / k,\n",
    "    \"eval_token_precision\": sum([m[\"eval_token_precision\"] for m in eval_metrics]) / k,\n",
    "    \"eval_token_recall\": sum([m[\"eval_token_recall\"] for m in eval_metrics]) / k,\n",
    "    \"eval_token_accuracy\": sum([m[\"eval_token_accuracy\"] for m in eval_metrics]) / k,\n",
    "    \"eval_span_f1\": sum([m[\"eval_span_f1\"] for m in eval_metrics]) / k,\n",
    "    \"eval_span_precision\": sum([m[\"eval_span_precision\"] for m in eval_metrics]) / k,\n",
    "    \"eval_span_recall\": sum([m[\"eval_span_recall\"] for m in eval_metrics]) / k,\n",
    "    \"eval_span_accuracy\": sum([m[\"eval_span_accuracy\"] for m in eval_metrics]) / k,\n",
    "    \"eval_runtime\": sum([m[\"eval_runtime\"] for m in eval_metrics]) / k,\n",
    "    \"eval_samples_per_second\": sum([m[\"eval_samples_per_second\"] for m in eval_metrics]) / k,\n",
    "    \"eval_steps_per_second\": sum([m[\"eval_steps_per_second\"] for m in eval_metrics]) / k,\n",
    "    \"epoch\": sum([m[\"epoch\"] for m in eval_metrics]) / k,\n",
    "}\n",
    "pd.DataFrame(eval_metrics_avg, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c:\\\\Users\\\\Vitalii\\\\Desktop\\\\5_course\\\\UNLP\\\\unlp-2025-manipulation-detector\\\\notebooks\\\\..\\\\models\\\\manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\\\\tokenizer_config.json',\n",
       " 'c:\\\\Users\\\\Vitalii\\\\Desktop\\\\5_course\\\\UNLP\\\\unlp-2025-manipulation-detector\\\\notebooks\\\\..\\\\models\\\\manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\\\\special_tokens_map.json',\n",
       " 'c:\\\\Users\\\\Vitalii\\\\Desktop\\\\5_course\\\\UNLP\\\\unlp-2025-manipulation-detector\\\\notebooks\\\\..\\\\models\\\\manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\\\\sentencepiece.bpe.model',\n",
       " 'c:\\\\Users\\\\Vitalii\\\\Desktop\\\\5_course\\\\UNLP\\\\unlp-2025-manipulation-detector\\\\notebooks\\\\..\\\\models\\\\manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\\\\added_tokens.json',\n",
       " 'c:\\\\Users\\\\Vitalii\\\\Desktop\\\\5_course\\\\UNLP\\\\unlp-2025-manipulation-detector\\\\notebooks\\\\..\\\\models\\\\manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\\\\tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(MODELS_FOLDER / \"manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\")\n",
    "tokenizer.save_pretrained(MODELS_FOLDER / \"manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail_2e-5_5epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODELS_FOLDER / \"manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODELS_FOLDER / \"manipulation-detector-xlm-roberta-base-ukrainian-ner-ukrner_exclude_tail\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "test_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_result = test_pipeline(dataset[\"test\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 383\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "visualize_as_markdown_and_save(\n",
    "    dataset[\"test\"],\n",
    "    test_result,\n",
    "    tokenizer,\n",
    "    REPORTS_FOLDER / \"span-detection\" / \"test-visualization\" / f\"test-{epoch_time}.md\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
