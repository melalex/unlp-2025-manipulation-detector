{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base BERT implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\Vitalii\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--seqeval\\541ae017dc683f85116597d48f621abc7b21b88dc42ec937c71af5415f0af63c (last modified on Sat Feb  8 15:26:43 2025) since it couldn't be found locally at evaluate-metric--seqeval, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.util.torch_device import resolve_torch_device\n",
    "from src.data.span_detection_ds import ManipulationDetectionDataset\n",
    "from src.definitions import (\n",
    "    MODELS_FOLDER,\n",
    "    RAW_DATA_FOLDER,\n",
    "    PROCESSED_DATA_FOLDER,\n",
    ")\n",
    "from src.model.span_detection_metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "device = resolve_torch_device()\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "\n",
    "epoch_time = int(time.time())\n",
    "\n",
    "#os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3439\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 383\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "dataset_blueprint = ManipulationDetectionDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    raw_path=RAW_DATA_FOLDER / \"span-detection.parquet\",\n",
    "    processed_path=PROCESSED_DATA_FOLDER / \"span-detection\",\n",
    "    seed=random_seed,\n",
    ")\n",
    "\n",
    "dataset = dataset_blueprint.read()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 1 testing encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            data[\"content\"],\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "        )\n",
    "        labels = []\n",
    "        \n",
    "       \n",
    "        for i, offsets in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "            example_labels = [0] * len(offsets)\n",
    "            trigger_words = data[\"trigger_words\"][i]\n",
    "            trigger_words = trigger_words if trigger_words is not None else []\n",
    "            for start, end in trigger_words:\n",
    "                for idx, (offset_start, offset_end) in enumerate(offsets):\n",
    "                    if offset_start >= start and offset_end <= end:\n",
    "                        example_labels[idx] = 1\n",
    "\n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "\n",
    "            previous_word_id = None\n",
    "\n",
    "            for j, id in enumerate(word_ids):\n",
    "                if id is None or id == previous_word_id:\n",
    "                    example_labels[j] = -100\n",
    "                previous_word_id = id\n",
    "\n",
    "            labels.append(example_labels)\n",
    "        \n",
    "        for i, offsets in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "            for token_id, offset, label, token in zip(tokenized_inputs[\"input_ids\"][i], offsets, labels[i], tokenized_inputs.tokens(i)):\n",
    "                print(token_id, offset, label, token)\n",
    "\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "        del tokenized_inputs[\"offset_mapping\"]\n",
    "\n",
    "        return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'content', 'lang', 'manipulative', 'techniques', 'trigger_words'],\n",
       "    num_rows: 3822\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "original = load_dataset(\n",
    "            \"parquet\", split=\"train\", data_files=str(RAW_DATA_FOLDER / \"span-detection.parquet\")\n",
    "        )\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0bb0c7fa-101b-4583-a5f9-9d503339141c',\n",
       " 'content': 'Новий огляд мапи DeepState від російського військового експерта, кухара путіна 2 розряду, спеціаліста по снарядному голоду та ректора музичної академії міноборони рф Євгєнія Пригожина. \\nПригожин прогнозує, що невдовзі настане день звільнення Криму і день розпаду росії. Каже, що передумови цього вже створені. \\n*Відео взяли з каналу \\nФД\\n. \\n@informnapalm',\n",
       " 'lang': 'uk',\n",
       " 'manipulative': True,\n",
       " 'techniques': ['euphoria', 'loaded_language'],\n",
       " 'trigger_words': [[27, 63], [65, 88], [90, 183], [186, 308]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 (0, 0) -100 [CLS]\n",
      "100325 (0, 5) 0 Новий\n",
      "555 (6, 7) 0 о\n",
      "41824 (7, 11) -100 ##гляд\n",
      "97744 (12, 14) 0 ма\n",
      "20785 (14, 16) -100 ##пи\n",
      "18891 (17, 21) 0 Deep\n",
      "10731 (21, 22) -100 ##S\n",
      "20359 (22, 26) -100 ##tate\n",
      "11141 (27, 30) 1 від\n",
      "28171 (31, 34) 1 рос\n",
      "44033 (34, 42) -100 ##ійського\n",
      "90602 (43, 54) 1 військового\n",
      "546 (55, 56) 1 е\n",
      "18705 (56, 58) -100 ##кс\n",
      "29633 (58, 61) -100 ##пер\n",
      "10367 (61, 63) -100 ##та\n",
      "117 (63, 64) 0 ,\n",
      "551 (65, 66) 1 к\n",
      "88081 (66, 69) -100 ##уха\n",
      "11079 (69, 71) -100 ##ра\n",
      "38675 (72, 75) 1 пут\n",
      "30487 (75, 78) -100 ##іна\n",
      "123 (79, 80) 1 2\n",
      "557 (81, 82) 1 р\n",
      "44666 (82, 84) -100 ##оз\n",
      "80367 (84, 88) -100 ##ряду\n",
      "117 (88, 89) 0 ,\n",
      "558 (90, 91) 1 с\n",
      "19820 (91, 93) -100 ##пе\n",
      "12167 (93, 95) -100 ##ці\n",
      "26983 (95, 98) -100 ##алі\n",
      "15535 (98, 101) -100 ##ста\n",
      "10297 (102, 104) 1 по\n",
      "558 (105, 106) 1 с\n",
      "37235 (106, 109) -100 ##нар\n",
      "35528 (109, 111) -100 ##яд\n",
      "15575 (111, 115) -100 ##ному\n",
      "92178 (116, 120) 1 голо\n",
      "15986 (120, 122) -100 ##ду\n",
      "10475 (123, 125) 1 та\n",
      "70158 (126, 129) 1 рек\n",
      "24425 (129, 133) -100 ##тора\n",
      "12388 (134, 136) 1 му\n",
      "14894 (136, 138) -100 ##зи\n",
      "26138 (138, 142) -100 ##чної\n",
      "55187 (143, 151) 1 академії\n",
      "553 (152, 153) 1 м\n",
      "14458 (153, 155) -100 ##ін\n",
      "33276 (155, 157) -100 ##об\n",
      "97648 (157, 160) -100 ##оро\n",
      "10656 (160, 162) -100 ##ни\n",
      "557 (163, 164) 1 р\n",
      "13582 (164, 165) -100 ##ф\n",
      "498 (166, 167) 1 Є\n",
      "10541 (167, 168) -100 ##в\n",
      "10823 (168, 169) -100 ##г\n",
      "12283 (169, 170) -100 ##є\n",
      "38033 (170, 173) -100 ##нія\n",
      "14337 (174, 177) 1 При\n",
      "10990 (177, 179) -100 ##го\n",
      "42589 (179, 183) -100 ##жина\n",
      "119 (183, 184) 0 .\n",
      "14337 (186, 189) 1 При\n",
      "10990 (189, 191) -100 ##го\n",
      "64800 (191, 194) -100 ##жин\n",
      "12709 (195, 198) 1 про\n",
      "10823 (198, 199) -100 ##г\n",
      "10636 (199, 201) -100 ##но\n",
      "19692 (201, 203) -100 ##зу\n",
      "12283 (203, 204) -100 ##є\n",
      "117 (204, 205) 1 ,\n",
      "10864 (206, 208) 1 що\n",
      "10375 (209, 211) 1 не\n",
      "10541 (211, 212) -100 ##в\n",
      "30470 (212, 215) -100 ##дов\n",
      "22551 (215, 217) -100 ##зі\n",
      "32001 (218, 221) 1 нас\n",
      "15061 (221, 224) -100 ##тан\n",
      "10205 (224, 225) -100 ##е\n",
      "16276 (226, 230) 1 день\n",
      "21107 (231, 233) 1 зв\n",
      "70510 (233, 236) -100 ##іль\n",
      "23299 (236, 241) -100 ##нення\n",
      "106175 (242, 247) 1 Криму\n",
      "579 (248, 249) 1 і\n",
      "16276 (250, 254) 1 день\n",
      "557 (255, 256) 1 р\n",
      "44666 (256, 258) -100 ##оз\n",
      "81017 (258, 262) -100 ##паду\n",
      "28171 (263, 266) 1 рос\n",
      "12890 (266, 268) -100 ##ії\n",
      "119 (268, 269) 1 .\n",
      "519 (270, 271) 1 К\n",
      "55522 (271, 273) -100 ##аж\n",
      "10205 (273, 274) -100 ##е\n",
      "117 (274, 275) 1 ,\n",
      "10864 (276, 278) 1 що\n",
      "18731 (279, 284) 1 перед\n",
      "20392 (284, 286) -100 ##ум\n",
      "15683 (286, 289) -100 ##ови\n",
      "17538 (290, 295) 1 цього\n",
      "23692 (296, 299) 1 вже\n",
      "15888 (300, 302) 1 ст\n",
      "29973 (302, 305) -100 ##вор\n",
      "21118 (305, 308) -100 ##ені\n",
      "119 (308, 309) 0 .\n",
      "115 (311, 312) 0 *\n",
      "52162 (312, 315) 0 Від\n",
      "26243 (315, 317) -100 ##ео\n",
      "82868 (318, 323) 0 взяли\n",
      "548 (324, 325) 0 з\n",
      "41308 (326, 331) 0 канал\n",
      "10227 (331, 332) -100 ##у\n",
      "529 (334, 335) 0 Ф\n",
      "22681 (335, 336) -100 ##Д\n",
      "119 (337, 338) 0 .\n",
      "137 (340, 341) 0 @\n",
      "26978 (341, 345) 0 info\n",
      "19341 (345, 347) -100 ##rm\n",
      "57992 (347, 350) -100 ##nap\n",
      "10415 (350, 352) -100 ##al\n",
      "10147 (352, 353) -100 ##m\n",
      "102 (0, 0) -100 [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoded = encode_labels(original[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODELS_FOLDER / \"manipulation-detector-bert-ner-checkpoint\"),\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    seed=random_seed,\n",
    "    logging_steps=200,\n",
    "    auto_find_batch_size=True,\n",
    "    torch_empty_cache_steps=1000,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(dataset_blueprint.label2id),\n",
    "    id2label=dataset_blueprint.id2label,\n",
    "    label2id=dataset_blueprint.label2id,\n",
    "    dropout=0.6,\n",
    ")\n",
    "\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {}\n",
    "\n",
    "\n",
    "def model_init(trial):\n",
    "    if trial:\n",
    "        config.update(\n",
    "            {\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.8, log=True),\n",
    "            }\n",
    "        )\n",
    "        training_args.learning_rate = trial.suggest_float(\n",
    "            \"learning_rate\", 1e-6, 1e-4, log=True\n",
    "        )\n",
    "        training_args.weight_decay = trial.suggest_float(\n",
    "            \"weight_decay\", 0.01, 0.9, log=True\n",
    "        )\n",
    "\n",
    "    return AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics(dataset_blueprint),\n",
    "    model_init=model_init,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run HP search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 23:50:40,349] A new study created in memory with name: no-name-63e3659c-47c3-4d85-bce4-76c69c303672\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 02:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.495145</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>0.749054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 23:52:55,821] Trial 0 finished with value: 0.011134307585247043 and parameters: {'dropout': 0.2153605042391882, 'learning_rate': 3.668086206434241e-06, 'weight_decay': 0.1757935022534685}. Best is trial 0 with value: 0.011134307585247043.\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 02:08, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>0.458914</td>\n",
       "      <td>0.035126</td>\n",
       "      <td>0.040661</td>\n",
       "      <td>0.037691</td>\n",
       "      <td>0.763192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 23:55:06,627] Trial 1 finished with value: 0.037691401648998826 and parameters: {'dropout': 0.3148451788551006, 'learning_rate': 3.3284573907694685e-05, 'weight_decay': 0.1813842407190132}. Best is trial 1 with value: 0.037691401648998826.\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 02:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.458164</td>\n",
       "      <td>0.034126</td>\n",
       "      <td>0.041931</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>0.762888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 23:57:14,248] Trial 2 finished with value: 0.03762827822120867 and parameters: {'dropout': 0.3027022659633645, 'learning_rate': 2.9222558191565536e-05, 'weight_decay': 0.7519791605815216}. Best is trial 1 with value: 0.037691401648998826.\n"
     ]
    }
   ],
   "source": [
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_f1\"]\n",
    "\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=30,\n",
    "    compute_objective=compute_objective,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=0.037691401648998826, hyperparameters={'dropout': 0.3148451788551006, 'learning_rate': 3.3284573907694685e-05, 'weight_decay': 0.1813842407190132}, run_summary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
